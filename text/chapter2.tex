\section{RBF 신경망}  

\subsection{소개}
RBF 신경망은 방사형 기저 함수 (RBF, Radial Basis Function)로 표현되는 뉴런으로 구성된 신경망입니다. 이 신경망은 미지의 함수 f에 대한 근사값을 입력 데이터로 받아 출력으로 미지의 함수 f에 대한 근사 함수를 생성할 수 있습니다. RBF는 가중치와 상호연결된 중심 값 \(X_{t}\)를 갖는데, 입력 값이 은닉층의 이 RBF를 통과하고 계수와 곱해져 출력층에서 선형적으로 합산되는 방식으로 근사치가 생성됩니다. \cite{574355} 또한 중심 값이 미리 정해지면 단일 은닉층의 각각의 노드가 RBF에 의한 지정된 비선형 변환을 수행하는 1-계층 신경망과 유사하게 볼 수 있습니다. \cite{298229}

\subsection{구현}
RBF에 사용되는 방사형 벡터 \(h\)가 다변량 가우시안 함수를 채택한다고 가정했을 때, 방사형 벡터의 각 요소는 식 \ref{eq_1}과 같이 나타낼 수 있습니다. \cite{1563016} 이 때  \(X\)는 입력 벡터, \(C\)는 중심 값 벡터, \(B\)는 방사형 너비 벡터입니다.
%
\begin{equation} \label{eq_1}
h_{j}=\exp(-{\left\Vert X-C_{j}\right\Vert^{2}\over 2b_{j}^{2}})
\end{equation}
%
그다음 신경망의 가중치 벡터 \(W\)는 식 \ref{eq_2}와 같이 나타낼 수 있습니다.
\begin{equation} \label{eq_2}
W=[w_{1},w_{2},\cdots w_{j}\cdots w_{m}]^{T} 
\end{equation}
%
신경망의 출력 \(y_m\)은 식 \ref{eq_3}과 같이 나타낼 수 있습니다.
%
\begin{equation} \label{eq_3}
y_{m}(k)=w_{0}+\sum\limits_{j=1}^{m}w_{j}h_{j} 
\end{equation}
%
경사하강법에 따라 가중치, 중심 값, 방사형 너비를 갱신하는 반복 알고리즘은 식 \ref{eq_4}, \ref{eq_5}, \ref{eq_6}과 같습니다. 이때 \(\alpha\)는 모멘텀 항으로 (모멘텀 gene이라고도 불림) 이전 가중치 갱신 반복이 다음 반복에 미치는 영향력을 결정하는 데 사용하며 \cite{Markopoulos2016-il} \(\eta\)는 학습률을 의미하고 \(y_{k}\)은 이후 언급되는 PID 제어기의 출력 또는 플랜트의 출력을 의미합니다. \((y_{k}=u)\)
%
\begin{align} 
\Delta W_{j} &=\alpha \{y_{k}(k)-y_{m}(k)\}h_{j} \\
\label{eq_4}
W_{j}(k) &=W_{j}(k-1)+\Delta W_{j}+\eta \{ W_{j}(k-1)-W_{j}(k-2) \} \\
\Delta B_{j} &=\alpha \{y_{k}(k)-y_{m}(k)\} W_{j}(k)h_{j} \\
\label{eq_5}
B_{j}(k) &= B_{j}(k-1)+\Delta B_{j} + \eta \{ B_{j}(k-1)-B_{j}(k-2) \} \\
\Delta C_{ji} &= \alpha \{ y_{k}(k)-y_{m}(k)\} W_{j}(k) \frac{X_{i}-C_{ji}(k)}{B_{j}(k)^2} \\
\label{eq_6}
C_{ji}(k) &= C_{ji}(k-1) + \Delta C_{ji} + \eta \{ C_{ji}(k-1)-C_{ji}(k-2) \}
\end{align} 
%
플랜트 입력에 대한 출력의 민감도를 의미하는 야코비안은 식 \ref{eq_7}과 같이 구합니다.
%
\begin{equation} \label{eq_7}
J = {\partial y_{k}\over \partial u}\approx{\partial y_{m}\over \partial u}
=\sum\limits_{j=1}^{m}W_{j}h_{j}{C_{ji}-x_{1}\over B_{j}^{2}}
\end{equation}
%
이때 \(x_{1}=u\)입니다.
%